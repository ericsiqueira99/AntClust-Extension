{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bfa61f-7e98-469d-a348-4d61c8813f96",
   "metadata": {},
   "source": [
    "# Clustering of MNIST \n",
    "\n",
    "In this example we try to cluster images from MNIST.  \n",
    "\n",
    "The idea is that it can cluster images from different classes using a cosine similarity from a pretrained CLIP model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8cf29-fd1a-4b8e-9038-b1e59552630d",
   "metadata": {},
   "source": [
    "### Data functions\n",
    "These functions are needed to load the images from the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fe4fea-be52-48a4-8b18-fa63f70a8717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\erics\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Balanced Subset Data Shape: (1000, 28, 28)\n",
      "Balanced Subset Targets Shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# Importing the dataset from keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "def extract_balanced_subset(data, targets, num_samples_per_class):\n",
    "    subset_data = []\n",
    "    subset_targets = []\n",
    "    class_samples_count = {label: 0 for label in range(10)}\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        label = int(targets[i][0])\n",
    "        if class_samples_count[label] < num_samples_per_class:\n",
    "            subset_data.append(data[i])\n",
    "            subset_targets.append(label)\n",
    "            class_samples_count[label] += 1\n",
    "\n",
    "            if all(count == num_samples_per_class for count in class_samples_count.values()):\n",
    "                break\n",
    "\n",
    "    return np.array(subset_data), np.array(subset_targets)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Flatten labels\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "\n",
    "# Shuffle the dataset\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
    "\n",
    "# Convert NumPy arrays to tuples for dictionary keys\n",
    "y_train_labels = [tuple(label) for label in y_train]\n",
    "\n",
    "# Set the number of samples you want for each class in the subset\n",
    "num_samples_per_class = 100\n",
    "\n",
    "# Extract the balanced subset\n",
    "balanced_subset_data, balanced_subset_targets = extract_balanced_subset(x_train, y_train_labels, num_samples_per_class)\n",
    "\n",
    "# Print the shapes of the balanced subset\n",
    "print(\"Balanced Subset Data Shape:\", balanced_subset_data.shape)\n",
    "print(\"Balanced Subset Targets Shape:\", balanced_subset_targets.shape)\n",
    "x_train = balanced_subset_data\n",
    "y_train = balanced_subset_targets\n",
    "true_labels = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291b511-9f21-4d14-91cf-4f60c5edcf24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AntClust Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7678567c-35d6-4754-9a9a-d1024c915d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b5d712aa7444e3af3f626902a79913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------\n",
    "#       imports\n",
    "# ----------------------\n",
    "# import opencv\n",
    "import cv2 as cv\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "# make AntClus dir known\n",
    "import sys\n",
    "sys.path.append(\"../AntClust\")\n",
    "# import AntClust\n",
    "from AntClust import AntClust\n",
    "# import the precomputed distance matrix function for AntClust\n",
    "import distance_classes\n",
    "reload(distance_classes)\n",
    "\n",
    "# import the rule set\n",
    "from rules import labroche_rules\n",
    "\n",
    "# ----------------------\n",
    "#       data\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "#       AntClust\n",
    "# ----------------------\n",
    "# tell AntClust to treat the data set as precomputed similarity matrix\n",
    "# similarity function\n",
    "f_sim = [distance_classes.image_cosine_similarity(img_tensor=x_train)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d9df93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntClust: phase 1 of 3 -> meeting ants\n",
      "Meeting 75000 / 75000\n",
      "Meeting 67500 / 75000\n",
      "Meeting 60000 / 75000\n",
      "Meeting 52500 / 75000\n",
      "Meeting 45000 / 75000\n",
      "Meeting 37500 / 75000\n",
      "Meeting 30000 / 75000\n",
      "Meeting 22500 / 75000\n",
      "Meeting 15000 / 75000\n",
      "Meeting 7500 / 75000\n",
      "AntClust: phase 2 of 3 -> shrink nests\n",
      "AntClust: phase 3 of 3 -> reassign ants\n"
     ]
    }
   ],
   "source": [
    "ant_clust = AntClust(f_sim, labroche_rules())\n",
    "\n",
    "# find clusters by using the distance matrix of the data\n",
    "\n",
    "ant_clust.fit([[i] for i in range(len(x_train))])\n",
    "\n",
    "# get the clustering result\n",
    "clusters_found = ant_clust.labels_\n",
    "clusters_found_cos = ant_clust.get_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e3a28-f5af-4214-a7f7-87fe4cfbaf6f",
   "metadata": {},
   "source": [
    "# K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd170368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "# Conversion to float\n",
    "x_train = x_train.astype('float32') \n",
    "# Normalization\n",
    "x_train = x_train/255.0\n",
    "X_train = x_train.reshape(len(x_train),-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea20ea07-43a8-4c5a-9146-346db3d9182d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "total_clusters = len(np.unique(y_test))\n",
    "# Initialize the K-Means model\n",
    "kmeans = KMeans(n_clusters = total_clusters)\n",
    "# Fitting the model to training set\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5ad4b",
   "metadata": {},
   "source": [
    "# AntClust ORB Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c6ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "#       imports\n",
    "# ----------------------\n",
    "# import sklearn distance function\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "# import opencv\n",
    "import cv2 as cv\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make AntClus dir known\n",
    "import sys\n",
    "sys.path.append(\"../AntClust\")\n",
    "# import AntClust\n",
    "from AntClust import AntClust\n",
    "# import the precomputed distance matrix function for AntClust\n",
    "from distance_classes import precomputed_similarity_matrix, opencv_orb_similarity\n",
    "# import the rule set\n",
    "from rules import labroche_rules\n",
    "\n",
    "def compute_orb_image_features(images):\n",
    "    \"\"\"Computes and returns the OpenCV ORB image feature descriptors\"\"\"\n",
    "    # Initiate ORB detector for feature extraction\n",
    "    orb = cv.ORB_create(nfeatures = 200, scaleFactor=1.2,nlevels=8,edgeThreshold=10,patchSize=16)\n",
    "    descriptors = []\n",
    "    not_index = []\n",
    "    # compute key points and descriptors\n",
    "    for i, image in enumerate(images):\n",
    "        #gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        kp, des = orb.detectAndCompute(image, None)\n",
    "        if des is not None:\n",
    "            descriptors.append([des])\n",
    "        else: \n",
    "            not_index.append(i)\n",
    "    return descriptors, not_index\n",
    "\n",
    "# ----------------------\n",
    "#       data\n",
    "# ----------------------\n",
    "image_data = x_train\n",
    "#image_data = read_images_from_array(image_data)\n",
    "#image_data = [[np.array(sub_array, dtype=np.uint8) for sub_array in array] for array in image_data]\n",
    "image_orbs, not_index = compute_orb_image_features(image_data)\n",
    "# print(image_orbs)\n",
    "data = np.array(image_orbs, dtype=list)\n",
    "labels = np.array(true_labels)\n",
    "# distance matrix for sklearn\n",
    "orb_sim = opencv_orb_similarity()\n",
    "distance_matrix = []\n",
    "for i in range(len(data)):\n",
    "    t_l = []\n",
    "    for n in range(len(data)):\n",
    "        t_l.append(orb_sim.similarity(data[i][0], data[n][0]))\n",
    "    distance_matrix.append(t_l)\n",
    "print(len(distance_matrix))\n",
    "# sklearn needs it in the way that 0 means a==b\n",
    "# ant clust needs it in the way 1 means a==b\n",
    "distance_matrix = 1 - np.array(distance_matrix)\n",
    "# AntClust needs every data tuple as an array.\n",
    "# e.g. [1,2,3] needs to be [[1],[2],[3]]\n",
    "distance_matrix = [[i] for i in distance_matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae33c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntClust: phase 1 of 3 -> meeting ants\n",
      "Meeting 69825 / 69825\n",
      "Meeting 55860 / 69825\n",
      "Meeting 41895 / 69825\n",
      "Meeting 27930 / 69825\n",
      "Meeting 13965 / 69825\n",
      "AntClust: phase 2 of 3 -> shrink nests\n",
      "AntClust: phase 3 of 3 -> reassign ants\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "#       AntClust\n",
    "# ----------------------\n",
    "# tell AntClust to treat the data set as precomputed similarity matrix\n",
    "# similarity function\n",
    "f_sim = [precomputed_similarity_matrix()]\n",
    "ant_clust = AntClust(f_sim, labroche_rules())\n",
    "\n",
    "# find clusters by using the distance matrix of the data\n",
    "ant_clust.fit(distance_matrix)\n",
    "\n",
    "# get the clustering result\n",
    "clusters_found_orb = ant_clust.get_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a774d",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f60ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI for AntClust (Cosine Similarity) 0.12179987677390727\n",
      "ARI for AntClust (ORB Similarity) 0.059549661048787034\n",
      "ARI for K-means (k=10) 0.34392891199081566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "ari_ant_cos = adjusted_rand_score(true_labels, clusters_found)\n",
    "ari_kmeans = adjusted_rand_score(true_labels, kmeans.labels_)\n",
    "filtered_labels = [true_labels[i] for i in range(len(true_labels)) if i not in not_index]\n",
    "ari_ant_orb = adjusted_rand_score(filtered_labels, clusters_found_orb)\n",
    "\n",
    "print(f\"ARI for AntClust (Cosine Similarity) {ari_ant_cos}\")\n",
    "print(f\"ARI for AntClust (ORB Similarity) {ari_ant_orb}\")\n",
    "print(f\"ARI for K-means (k={total_clusters}) {ari_kmeans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e0862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
