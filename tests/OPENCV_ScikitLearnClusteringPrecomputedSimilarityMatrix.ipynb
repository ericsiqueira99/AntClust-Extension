{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa605041-262d-436b-aa09-9f0f88be232f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Notebook variables\n",
    "# # ---------------------\n",
    "# # should tests be runned?\n",
    "# run_simulations = True\n",
    "\n",
    "# # should the test data be saved?\n",
    "# save_simulation_data = True\n",
    "\n",
    "# # should the stored test data be loaded?\n",
    "# load_simulation_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d91aaea-20e7-44e4-aca4-cd8aa580ca05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "import math\n",
    "import os\n",
    "import random as rng\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# make AntClus dir known\n",
    "import sys\n",
    "sys.path.append(\"../AntClust\")\n",
    "from AntClust import AntClust\n",
    "from distance_classes import (\n",
    "    opencv_image_flann_similarity,\n",
    "    opencv_image_orb_similarity,\n",
    "    opencv_descriptor_flann_similarity,\n",
    "    opencv_orb_similarity,\n",
    "    precomputed_similarity_matrix\n",
    ")\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from rules import labroche_rules\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adadf7-426c-4d78-9fe5-edad31bc21a7",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915cc78a-8c29-4876-858a-8dbba200b44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_cluster_images_static(\n",
    "    data_folder, num_clusters, num_images_per_cluster, seed=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Will generate num_clusters clusters with num_images_per_cluster pictures each\n",
    "    \"\"\"\n",
    "    # get the cars\n",
    "    car_dir_names = sorted(os.listdir(data_folder), key=lambda x: x)\n",
    "\n",
    "    # remove ipycheckpoint\n",
    "    if car_dir_names[0] == \".ipynb_checkpoints\":\n",
    "        car_dir_names = car_dir_names[1::]\n",
    "\n",
    "    # make a random shuffle of the cars/folders to take\n",
    "    rng.seed(seed)\n",
    "    cars_to_take = rng.sample(car_dir_names, len(car_dir_names))\n",
    "    cars_to_take = cars_to_take[0:num_clusters:]\n",
    "\n",
    "    # make the cluster data and the labels\n",
    "    # generate num_clusters and add num_images_per_cluster to each cluster\n",
    "    cluster_image: list = []\n",
    "    cluster_labels: list = []\n",
    "\n",
    "    label_counter = 0\n",
    "    for car_folder in cars_to_take:\n",
    "        # take images and shuffle them\n",
    "        imgs = sorted(os.listdir(data_folder + \"/\" + car_folder), key=lambda x: x)\n",
    "        imgs = rng.sample(imgs, len(imgs))\n",
    "\n",
    "        # make data and labels\n",
    "        # add the respectiv car folder as path\n",
    "        cluster_image = cluster_image + [\n",
    "            str(car_folder) + \"/\" + image for image in imgs[0:num_images_per_cluster:]\n",
    "        ]\n",
    "        cluster_labels = cluster_labels + [label_counter] * num_images_per_cluster\n",
    "        label_counter += 1\n",
    "\n",
    "    # read the images as opencv images from disk\n",
    "    image_data = []\n",
    "    for image_file in cluster_image:\n",
    "        image_data.append(\n",
    "            [cv.imread(data_folder + \"/\" + image_file, cv.IMREAD_GRAYSCALE)]\n",
    "        )\n",
    "\n",
    "    return cluster_image, image_data, cluster_labels\n",
    "\n",
    "\n",
    "def data_cluster_images_dynamic(\n",
    "    data_folder,\n",
    "    num_clusters,\n",
    "    num_images_per_cluster_min,\n",
    "    num_images_per_cluster_max,\n",
    "    seed=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Will generate num_clusters clusters where a random ammount of images in\n",
    "    the range [num_images_per_cluster_min, num_images_per_cluster_max]\n",
    "    \"\"\"\n",
    "    # get the cars\n",
    "    car_dir_names = sorted(os.listdir(data_folder), key=lambda x: x)\n",
    "\n",
    "    # remove ipycheckpoint\n",
    "    if car_dir_names[0] == \".ipynb_checkpoints\":\n",
    "        car_dir_names = car_dir_names[1::]\n",
    "\n",
    "    # make a random shuffle of the cars/folders to take\n",
    "    rng.seed(seed)\n",
    "    cars_to_take = rng.sample(car_dir_names, len(car_dir_names))\n",
    "    cars_to_take = cars_to_take[0:num_clusters:]\n",
    "\n",
    "    # make the cluster data and the labels\n",
    "    # generate num_clusters and add a random ammount of images (in min, max range) to each cluster\n",
    "    cluster_image: list = []\n",
    "    cluster_labels: list = []\n",
    "\n",
    "    label_counter = 0\n",
    "    for car_folder in cars_to_take:\n",
    "        # take images and shuffle them\n",
    "        imgs = sorted(os.listdir(data_folder + \"/\" + car_folder), key=lambda x: x)\n",
    "        imgs = rng.sample(imgs, len(imgs))\n",
    "\n",
    "        # make data and labels\n",
    "        num_images = rng.randint(num_images_per_cluster_min, num_images_per_cluster_max)\n",
    "        # add the respectiv car folder as path\n",
    "        cluster_image = cluster_image + [\n",
    "            str(car_folder) + \"/\" + image for image in imgs[0:num_images:]\n",
    "        ]\n",
    "        cluster_labels = cluster_labels + [label_counter] * num_images\n",
    "        label_counter += 1\n",
    "\n",
    "    # read the images as opencv images from disk\n",
    "    # and put them into their own array as data tuple\n",
    "    image_data = []\n",
    "    for image_file in cluster_image:\n",
    "        image_data.append(\n",
    "            [cv.imread(data_folder + \"/\" + image_file, cv.IMREAD_GRAYSCALE)]\n",
    "        )\n",
    "\n",
    "    return cluster_image, image_data, cluster_labels\n",
    "\n",
    "\n",
    "def compute_orb_image_features(images, image_resize_size):\n",
    "    # Initiate ORB detector for feature extraction\n",
    "    orb = cv.ORB_create()\n",
    "    descriptors = []\n",
    "    # compute key points and descriptors\n",
    "    for image in images:\n",
    "        img = image[0]\n",
    "        img = cv.resize(img, image_resize_size)\n",
    "        kp, des = orb.detectAndCompute(img, None)\n",
    "        descriptors.append([des])\n",
    "\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30688b3-7343-4021-8774-022160fb80ba",
   "metadata": {},
   "source": [
    "# Cluster Performance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea4bc2b-fdd2-413e-a012-62988f7f44cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure performance on 2 clusters\n",
      "measure performance on 4 clusters\n",
      "measure performance on 6 clusters\n",
      "measure performance on 8 clusters\n",
      "measure performance on 10 clusters\n",
      "measure performance on 12 clusters\n",
      "measure performance on 14 clusters\n",
      "measure performance on 16 clusters\n",
      "measure performance on 18 clusters\n",
      "measure performance on 20 clusters\n",
      "measure performance on 22 clusters\n",
      "measure performance on 24 clusters\n",
      "measure performance on 26 clusters\n",
      "measure performance on 28 clusters\n",
      "measure performance on 30 clusters\n",
      "testing took 1083.333025932312 seconds\n"
     ]
    }
   ],
   "source": [
    "# test variables\n",
    "# --------------\n",
    "clusters_min = 2\n",
    "clusters_max = 30\n",
    "test_every_n = 2\n",
    "num_clusters = np.arange(clusters_min, clusters_max + 1, test_every_n)\n",
    "data_folder = \"data\"\n",
    "values_per_cluster = 18\n",
    "seed = 9\n",
    "image_resize_size = (150, 172)\n",
    "\n",
    "# AntClust standart params\n",
    "ant_clust_params = {'alpha': 500, 'betta': 0.9, 'shrink': 0.2, 'removal': 0.3}\n",
    "dbscan_params = {\"eps\": 0.33, \"min_samples\": 2}\n",
    "\n",
    "# result arrays\n",
    "ari_antclust = []\n",
    "ari_dbscan = []\n",
    "ari_hdbscan = []\n",
    "ari_optics = []\n",
    "\n",
    "# test loop\n",
    "t_0 = time.time()\n",
    "for num_clust in num_clusters:\n",
    "    print(f\"measure performance on {num_clust} clusters\")\n",
    "    # Data generation\n",
    "    # ----------------\n",
    "    image_names, images, labels = data_cluster_images_static(\n",
    "        data_folder, num_clust, values_per_cluster, seed\n",
    "    )\n",
    "    images = compute_orb_image_features(images, image_resize_size)\n",
    "    data = np.array(images, dtype=list)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # distance matrix for sklearn\n",
    "    orb_sim = opencv_orb_similarity()\n",
    "    distance_matrix = []\n",
    "    for i in range(len(data)):\n",
    "        t_l = []\n",
    "        for n in range(len(data)):\n",
    "            t_l.append(orb_sim.similarity(data[i][0], data[n][0]))\n",
    "        distance_matrix.append(t_l)\n",
    "    # sklearn needs it in the way that 0 means a==b\n",
    "    # ant clust needs it in the way 1 means a==b\n",
    "    data_distance_matrix = 1 - np.array(distance_matrix)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # clustering with different algorithms\n",
    "    # ------------------------------------\n",
    "    # AntClust\n",
    "    # ----------\n",
    "    # similarity function\n",
    "    f_sim = [precomputed_similarity_matrix()]\n",
    "    # rules\n",
    "    rules = labroche_rules()\n",
    "    # AntClust\n",
    "    ant_clust = AntClust(\n",
    "        f_sim,\n",
    "        rules,\n",
    "        alpha_ant_meeting_iterations=ant_clust_params[\"alpha\"],\n",
    "        betta_template_init_meetings=ant_clust_params[\"betta\"],\n",
    "        nest_shrink_prop=ant_clust_params[\"shrink\"],\n",
    "        nest_removal_prop=ant_clust_params[\"removal\"],\n",
    "        print_status=False,\n",
    "    )\n",
    "    # find clusters\n",
    "    ant_clust.fit([[i] for i in data_distance_matrix])\n",
    "    # get the clustering result\n",
    "    y_pred = ant_clust.get_clusters()\n",
    "    # calculate the ari score\n",
    "    ari_score = adjusted_rand_score(labels, y_pred)\n",
    "    # append score\n",
    "    ari_antclust.append(ari_score)\n",
    "\n",
    "    # DBSCAN\n",
    "    # ----------\n",
    "    dbscan = DBSCAN(\n",
    "        eps=dbscan_params[\"eps\"],\n",
    "        min_samples=dbscan_params[\"min_samples\"],\n",
    "        metric=\"precomputed\",\n",
    "    )\n",
    "\n",
    "    # Fit the model to the data\n",
    "    dbscan.fit(data_distance_matrix)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    y_pred = dbscan.labels_\n",
    "    # calculate the ari score\n",
    "    ari_score = adjusted_rand_score(labels, y_pred)\n",
    "    # append score\n",
    "    ari_dbscan.append(ari_score)\n",
    "\n",
    "    # HDBSCAN\n",
    "    # ----------\n",
    "    hdbscan = HDBSCAN(metric=\"precomputed\")\n",
    "\n",
    "    # Fit the model to the data\n",
    "    hdbscan.fit(data_distance_matrix)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    y_pred = hdbscan.labels_\n",
    "    # calculate the ari score\n",
    "    ari_score = adjusted_rand_score(labels, y_pred)\n",
    "    # append score\n",
    "    ari_hdbscan.append(ari_score)\n",
    "\n",
    "    # OPTICS\n",
    "    # ----------\n",
    "    optics = OPTICS(metric=\"precomputed\")\n",
    "\n",
    "    # Fit the model to the data\n",
    "    optics.fit(data_distance_matrix)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    y_pred = optics.labels_\n",
    "    # calculate the ari score\n",
    "    ari_score = adjusted_rand_score(labels, y_pred)\n",
    "    # append score\n",
    "    ari_optics.append(ari_score)\n",
    "print(f\"testing took {time.time() - t_0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b75c5c8-4317-43c3-978a-6bd55bd3af00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35610612924100954, 0.5635759252725213, 0.6769081571929206, 0.7577188882812111, 0.6392491088718132, 0.6592643929957883, 0.6811088451146746, 0.6635066743157172, 0.6418196702140101, 0.6580274648430187, 0.6030222029190946, 0.5730978909016121, 0.5834164097963406, 0.5017854815587124, 0.5638017432112977]\n",
      "[0.753633787366304, 0.7290871734663927, 0.741189192265555, 0.7306183296628833, 0.7214452446980428, 0.3379660600321533, 0.225671510253856, 0.20376583574899268, 0.17077394519743636, 0.20746875920288518, 0.15106708617020048, 0.1068482019577566, 0.08270926549569964, 0.05869973573641392, 0.06191467990423928]\n",
      "[0.8564397046759639, 0.7589267383648395, 0.5526232385781852, 0.7129780201561271, 0.645396570259579, 0.48741134454083845, 0.4538531680073583, 0.4770498270178605, 0.4102312836286759, 0.3981154175560774, 0.3559824314691303, 0.4089762910895465, 0.3757083892502588, 0.3539337353650682, 0.3865623728353755]\n",
      "[0.4468804238604184, 0.6251977631228559, 0.2850013806260512, 0.44954925869818957, 0.39000046010152906, 0.26783216417246514, 0.2804591178225645, 0.2393768124033248, 0.16139771729587357, 0.17036400564916504, 0.12273951211678205, 0.10271707465907105, 0.0937043476911139, 0.07600486540136016, 0.0856640223360947]\n"
     ]
    }
   ],
   "source": [
    "print(ari_antclust)\n",
    "print(ari_dbscan)\n",
    "print(ari_hdbscan)\n",
    "print(ari_optics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31034e6f-6d27-4ede-872b-d9915d11bdcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6081605989819828\n",
      "0.35219058714392065\n",
      "0.5089459021863256\n",
      "0.25312592839712394\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ari_antclust))\n",
    "print(np.mean(ari_dbscan))\n",
    "print(np.mean(ari_hdbscan))\n",
    "print(np.mean(ari_optics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_evolution",
   "language": "python",
   "name": "venv_evolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
