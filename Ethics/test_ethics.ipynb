{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Ethical Decision of Simple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ethic_Ant\n",
    "import Ethic_AntClust\n",
    "import Ethic_rules\n",
    "from afinn import Afinn\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "reload(Ethic_Ant)\n",
    "reload(Ethic_AntClust)\n",
    "reload(Ethic_rules)\n",
    "\n",
    "# Initialize AFINN\n",
    "afinn = Afinn()\n",
    "\n",
    "def normalize_list(input_list):\n",
    "    # Find the minimum and maximum values in the list\n",
    "    min_val = min(input_list)\n",
    "    max_val = max(input_list)\n",
    "    \n",
    "    # Normalize each value in the list\n",
    "    normalized_list = [(x - min_val) / (max_val - min_val) for x in input_list]\n",
    "    \n",
    "    return normalized_list\n",
    "\n",
    "# Example text\n",
    "tokens = [\"kill\", \"hurt\", \"steal\", \"lie\", \"hug\", \"love\", \"help\"]\n",
    "\n",
    "\n",
    "# Sentiment analysis and encoding\n",
    "sentiment_scores = [afinn.score(token) for token in tokens]\n",
    "sentiment_scores = normalize_list(sentiment_scores)\n",
    "\n",
    "# Example sentiment scores for tokens\n",
    "sentiment_scores_dict = dict(zip(tokens, sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ant\n",
    "def run_antclust(pop_size,sentiment_scores_dict):\n",
    "    # AntClust\n",
    "    result = {}\n",
    "    ant_clust_params={\"alpha\": 250, \"betta\": 0.5, \"shrink\": 0.2, \"removal\": 0.3}\n",
    "    ant_clust = Ethic_AntClust.AntClust(\n",
    "        pop_size,\n",
    "        Ethic_rules.ethical_rules(),\n",
    "        alpha_ant_meeting_iterations=ant_clust_params[\"alpha\"],\n",
    "        betta_template_init_meetings=ant_clust_params[\"betta\"],\n",
    "        nest_shrink_prop=ant_clust_params[\"shrink\"],\n",
    "        nest_removal_prop=ant_clust_params[\"removal\"],\n",
    "        print_status=False,\n",
    "    )\n",
    "    for key in sentiment_scores_dict.keys():\n",
    "        ant_clust.fit(sentiment_scores_dict[key])\n",
    "        clusters_found = ant_clust.get_clusters()\n",
    "        unique_values, counts = np.unique(clusters_found, return_counts=True)\n",
    "\n",
    "        # Find the index of the maximum count\n",
    "        max_count_index = np.argmax(counts)\n",
    "\n",
    "        # Create a dictionary to store the count of each value\n",
    "        count_dict = dict(zip(unique_values, counts))\n",
    "        #print(count_dict)\n",
    "        # Get the value(s) with the maximum count\n",
    "        majority_values = unique_values[counts == counts[max_count_index]]\n",
    "        decision = \"Good\" if majority_values == 1 else \"Bad\"\n",
    "        result[key] = decision\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kill': 'Bad', 'hurt': 'Bad', 'steal': 'Bad', 'lie': 'Bad', 'hug': 'Good', 'love': 'Bad', 'help': 'Good'}\n"
     ]
    }
   ],
   "source": [
    "ethical_decision = run_antclust(150,sentiment_scores_dict)\n",
    "print(ethical_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of full texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.375, 1.0, 0.375, 0.625, 1.0, 0.625, 0.75, 0.625, 0.625, 0.75, 0.5, 0.75, 0.625, 0.875, 0.0, 0.625, 1.0, 0.375, 0.625, 0.5]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The rain ruined our picnic, but the laughter warmed our hearts.\",\n",
    "    \"Despite the setbacks, we managed to find joy in each other's company.\",\n",
    "    \"The concert was electrifying, but the long wait in line was frustrating.\",\n",
    "    \"His words cut deep, yet his apology felt sincere.\",\n",
    "    \"The sunrise painted the sky with vibrant hues, lifting our spirits.\",\n",
    "    \"The traffic jam tested our patience, but the music kept us entertained.\",\n",
    "    \"She smiled through tears, finding solace in shared memories.\",\n",
    "    \"The party was a blast, despite the minor mishaps along the way.\",\n",
    "    \"The cold weather chilled our bones, but the hot cocoa warmed us up.\",\n",
    "    \"His criticism stung, overshadowing the praise he offered.\",\n",
    "    \"The new recipe was a disaster, but we laughed it off and ordered pizza.\",\n",
    "    \"The heartfelt apology melted away the tension between them.\",\n",
    "    \"The unexpected gift brought tears to her eyes, filling her with gratitude.\",\n",
    "    \"The movie was mediocre, but the company made it enjoyable.\",\n",
    "    \"Losing the game was disappointing, but the team's effort was commendable.\",\n",
    "    \"The flowers wilted, but their fragrance lingered, evoking nostalgia.\",\n",
    "    \"The beach was crowded, yet the sound of crashing waves was soothing.\",\n",
    "    \"Her infectious laughter brightened even the gloomiest of days.\",\n",
    "    \"The unexpected delay tested our patience, but we eventually arrived safely.\",\n",
    "    \"The argument escalated quickly, leaving both parties feeling hurt.\"\n",
    "]\n",
    "\n",
    "sentiment_scores_sentence = [afinn.score(token) for token in sentences]\n",
    "sentiment_scores_sentence = normalize_list(sentiment_scores_sentence)\n",
    "\n",
    "print(sentiment_scores_sentence)\n",
    "# Example sentiment scores for tokens\n",
    "sentiment_scores_sentence_dict = dict(zip(sentences, sentiment_scores_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethical_decision = run_antclust(150,sentiment_scores_sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rain ruined our picnic, but the laughter warmed our hearts. : Bad\n",
      "Despite the setbacks, we managed to find joy in each other's company. : Good\n",
      "The concert was electrifying, but the long wait in line was frustrating. : Bad\n",
      "His words cut deep, yet his apology felt sincere. : Bad\n",
      "The sunrise painted the sky with vibrant hues, lifting our spirits. : Good\n",
      "The traffic jam tested our patience, but the music kept us entertained. : Bad\n",
      "She smiled through tears, finding solace in shared memories. : Good\n",
      "The party was a blast, despite the minor mishaps along the way. : Good\n",
      "The cold weather chilled our bones, but the hot cocoa warmed us up. : Good\n",
      "His criticism stung, overshadowing the praise he offered. : Bad\n",
      "The new recipe was a disaster, but we laughed it off and ordered pizza. : Bad\n",
      "The heartfelt apology melted away the tension between them. : Good\n",
      "The unexpected gift brought tears to her eyes, filling her with gratitude. : Good\n",
      "The movie was mediocre, but the company made it enjoyable. : Good\n",
      "Losing the game was disappointing, but the team's effort was commendable. : Bad\n",
      "The flowers wilted, but their fragrance lingered, evoking nostalgia. : Good\n",
      "The beach was crowded, yet the sound of crashing waves was soothing. : Good\n",
      "Her infectious laughter brightened even the gloomiest of days. : Bad\n",
      "The unexpected delay tested our patience, but we eventually arrived safely. : Bad\n",
      "The argument escalated quickly, leaving both parties feeling hurt. : Bad\n"
     ]
    }
   ],
   "source": [
    "for key in ethical_decision.keys():\n",
    "    print(f\"{key} : {ethical_decision[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7142857142857143, 0.7142857142857143, 0.2857142857142857, 0.5714285714285714, 0.42857142857142855, 0.0, 0.7142857142857143, 0.14285714285714285, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.14285714285714285, 0.42857142857142855, 0.0, 0.14285714285714285, 0.2857142857142857, 0.5714285714285714, 0.42857142857142855, 1.0, 0.5714285714285714]\n",
      "I could manipulate the data to favor our results. : Bad\n",
      "Let's withhold this information from the public to maintain our advantage. : Good\n",
      "We could ignore safety regulations to cut costs. : Good\n",
      "We might exaggerate the benefits of our product to increase sales. : Good\n",
      "Let's underreport our income to minimize taxes. : Bad\n",
      "We could exploit loopholes in the law to avoid accountability. : Bad\n",
      "We might plagiarize content to save time. : Good\n",
      "Let's use misleading advertising to attract customers. : Good\n",
      "We could sabotage our competitors to gain an edge. : Good\n",
      "Let's bribe officials to speed up approval processes. : Bad\n",
      "We might discriminate against certain groups to maintain control. : Bad\n",
      "Let's manipulate public opinion through misinformation campaigns. : Bad\n",
      "We could engage in insider trading to profit illegally. : Bad\n",
      "Let's falsify credentials to appear more qualified. : Bad\n",
      "We might exploit vulnerable populations for profit. : Bad\n",
      "Let's deceive customers about the quality of our product. : Bad\n",
      "We could engage in price-fixing to eliminate competition. : Good\n",
      "Let's ignore environmental regulations to increase productivity. : Bad\n",
      "We might sell customer data without their consent for profit. : Good\n",
      "Let's silence whistleblowers to protect our reputation. : Bad\n"
     ]
    }
   ],
   "source": [
    "ethically_dubious_sentences = [\n",
    "    \"I could manipulate the data to favor our results.\",\n",
    "    \"Let's withhold this information from the public to maintain our advantage.\",\n",
    "    \"We could ignore safety regulations to cut costs.\",\n",
    "    \"We might exaggerate the benefits of our product to increase sales.\",\n",
    "    \"Let's underreport our income to minimize taxes.\",\n",
    "    \"We could exploit loopholes in the law to avoid accountability.\",\n",
    "    \"We might plagiarize content to save time.\",\n",
    "    \"Let's use misleading advertising to attract customers.\",\n",
    "    \"We could sabotage our competitors to gain an edge.\",\n",
    "    \"Let's bribe officials to speed up approval processes.\",\n",
    "    \"We might discriminate against certain groups to maintain control.\",\n",
    "    \"Let's manipulate public opinion through misinformation campaigns.\",\n",
    "    \"We could engage in insider trading to profit illegally.\",\n",
    "    \"Let's falsify credentials to appear more qualified.\",\n",
    "    \"We might exploit vulnerable populations for profit.\",\n",
    "    \"Let's deceive customers about the quality of our product.\",\n",
    "    \"We could engage in price-fixing to eliminate competition.\",\n",
    "    \"Let's ignore environmental regulations to increase productivity.\",\n",
    "    \"We might sell customer data without their consent for profit.\",\n",
    "    \"Let's silence whistleblowers to protect our reputation.\",\n",
    "]\n",
    "\n",
    "sentiment_scores_sentence = [afinn.score(token) for token in ethically_dubious_sentences]\n",
    "sentiment_scores_sentence = normalize_list(sentiment_scores_sentence)\n",
    "\n",
    "print(sentiment_scores_sentence)\n",
    "# Example sentiment scores for tokens\n",
    "sentiment_scores_sentence_dict = dict(zip(ethically_dubious_sentences, sentiment_scores_sentence))\n",
    "ethical_decision = run_antclust(150,sentiment_scores_sentence_dict)\n",
    "for key in ethical_decision.keys():\n",
    "    print(f\"{key} : {ethical_decision[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\erics\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66ce443b77849e99d3f980248713711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erics\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\erics\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afee13f09d26414284bbbf1e33e12408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8cf9785a8340918da84053c8c427c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba8799bcd304282a3377c34f18f5479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9998849630355835\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example sentence for sentiment analysis\n",
    "sentence = \"I really love this product. It's amazing!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I returned the wallet I found on the street to its owner.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.967136561870575\n",
      "I volunteered at a homeless shelter last weekend.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9709669947624207\n",
      "I helped an elderly person cross the street.\n",
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9920237064361572\n",
      "I donated blood at the local blood drive.\n",
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.5163185596466064\n",
      "I recycled all of my plastic bottles and paper products.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9976401329040527\n",
      "I adopted a rescue dog from the animal shelter.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.8940428495407104\n",
      "I stood up for a bullied classmate.\n",
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9862798452377319\n",
      "I donated money to a charity that supports education in underprivileged communities.\n",
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9004136919975281\n",
      "I stole money from my roommate's wallet.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9944221377372742\n",
      "I cheated on a test to get a better grade.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9996228218078613\n",
      "I spread false rumors about a coworker to make myself look better.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9992426633834839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I vandalized public property.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9993075132369995\n",
      "I discriminated against someone based on their race.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9891946315765381\n",
      "I plagiarized someone else's work and submitted it as my own.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.9968364238739014\n",
      "I drove under the influence of alcohol.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.994149923324585\n",
      "I harassed someone online.\n",
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.999422550201416\n"
     ]
    }
   ],
   "source": [
    "ethically_dubious_sentences = [\n",
    "    \"I could manipulate the data to favor our results.\",\n",
    "    \"Let's withhold this information from the public to maintain our advantage.\",\n",
    "    \"We could ignore safety regulations to cut costs.\",\n",
    "    \"We might exaggerate the benefits of our product to increase sales.\",\n",
    "    \"Let's underreport our income to minimize taxes.\",\n",
    "    \"We could exploit loopholes in the law to avoid accountability.\",\n",
    "    \"We might plagiarize content to save time.\",\n",
    "    \"Let's use misleading advertising to attract customers.\",\n",
    "    \"We could sabotage our competitors to gain an edge.\",\n",
    "    \"Let's bribe officials to speed up approval processes.\",\n",
    "    \"We might discriminate against certain groups to maintain control.\",\n",
    "    \"Let's manipulate public opinion through misinformation campaigns.\",\n",
    "    \"We could engage in insider trading to profit illegally.\",\n",
    "    \"Let's falsify credentials to appear more qualified.\",\n",
    "    \"We might exploit vulnerable populations for profit.\",\n",
    "    \"Let's deceive customers about the quality of our product.\",\n",
    "    \"We could engage in price-fixing to eliminate competition.\",\n",
    "    \"Let's ignore environmental regulations to increase productivity.\",\n",
    "    \"We might sell customer data without their consent for profit.\",\n",
    "    \"Let's silence whistleblowers to protect our reputation.\",\n",
    "]\n",
    "\n",
    "for sentence in ethically_sentences:\n",
    "    # Perform sentiment analysis\n",
    "    result = sentiment_pipeline(sentence)\n",
    "\n",
    "    # Extract sentiment label and score from the result\n",
    "    sentiment_label = result[0]['label']\n",
    "    sentiment_score = result[0]['score']\n",
    "    \n",
    "    print(sentence)\n",
    "    print(\"Sentiment:\", sentiment_label)\n",
    "    print(\"Confidence Score:\", sentiment_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I returned the wallet I found on the street to its owner.\n",
      "Sentiment Score: 0.0\n",
      "I volunteered at a homeless shelter last weekend.\n",
      "Sentiment Score: 0.0\n",
      "I helped an elderly person cross the street.\n",
      "Sentiment Score: 0.0\n",
      "I donated blood at the local blood drive.\n",
      "Sentiment Score: 0.0\n",
      "I recycled all of my plastic bottles and paper products.\n",
      "Sentiment Score: 0.0\n",
      "I adopted a rescue dog from the animal shelter.\n",
      "Sentiment Score: 0.5106\n",
      "I stood up for a bullied classmate.\n",
      "Sentiment Score: -0.6249\n",
      "I donated money to a charity that supports education in underprivileged communities.\n",
      "Sentiment Score: 0.6486\n",
      "I stole money from my roommate's wallet.\n",
      "Sentiment Score: 0.0\n",
      "I cheated on a test to get a better grade.\n",
      "Sentiment Score: -0.1027\n",
      "I spread false rumors about a coworker to make myself look better.\n",
      "Sentiment Score: 0.4404\n",
      "I vandalized public property.\n",
      "Sentiment Score: 0.0\n",
      "I discriminated against someone based on their race.\n",
      "Sentiment Score: 0.0\n",
      "I plagiarized someone else's work and submitted it as my own.\n",
      "Sentiment Score: 0.0\n",
      "I drove under the influence of alcohol.\n",
      "Sentiment Score: 0.0\n",
      "I harassed someone online.\n",
      "Sentiment Score: -0.5423\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example sentence for sentiment analysis\n",
    "\n",
    "ethically_sentences = [\n",
    "    \"I returned the wallet I found on the street to its owner.\",\n",
    "    \"I volunteered at a homeless shelter last weekend.\",\n",
    "    \"I helped an elderly person cross the street.\",\n",
    "    \"I donated blood at the local blood drive.\",\n",
    "    \"I recycled all of my plastic bottles and paper products.\",\n",
    "    \"I adopted a rescue dog from the animal shelter.\",\n",
    "    \"I stood up for a bullied classmate.\",\n",
    "    \"I donated money to a charity that supports education in underprivileged communities.\",\n",
    "    \"I stole money from my roommate's wallet.\",\n",
    "    \"I cheated on a test to get a better grade.\",\n",
    "    \"I spread false rumors about a coworker to make myself look better.\",\n",
    "    \"I vandalized public property.\",\n",
    "    \"I discriminated against someone based on their race.\",\n",
    "    \"I plagiarized someone else's work and submitted it as my own.\",\n",
    "    \"I drove under the influence of alcohol.\",\n",
    "    \"I harassed someone online.\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in ethically_sentences: \n",
    "    # Perform sentiment analysis\n",
    "    sentiment_scores = analyzer.polarity_scores(sentence)\n",
    "\n",
    "    # Extract sentiment score\n",
    "    sentiment_score = sentiment_scores['compound']\n",
    "    print(sentence)\n",
    "    print(\"Sentiment Score:\", sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
